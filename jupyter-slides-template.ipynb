{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Testing for Ghosts in the Machine: Assuring 'Good Enough' Software Quality in AI-based systems<br>\n",
    "\n",
    "## Artur Patoka\n",
    "PyCon Italia<br>\n",
    "Florence 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![prace_logos.png](https://raw.githubusercontent.com/arturpat/pycon-it-pres-2/main/prace_logos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Problem statement (the WHY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The challenges (the OH MY GOD üò±)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The solutions (the relief üòå)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![1.png](https://raw.githubusercontent.com/arturpat/pycon-it-pres-2/main/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![2.png](https://raw.githubusercontent.com/arturpat/pycon-it-pres-2/main/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![3.png](https://raw.githubusercontent.com/arturpat/pycon-it-pres-2/main/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![4.GIF](https://raw.githubusercontent.com/arturpat/pycon-it-pres-2/main/4.GIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What about some real-life examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![air_canada.png](https://raw.githubusercontent.com/arturpat/pycon-it-pres-2/main/air_canada.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![chevy.png](https://raw.githubusercontent.com/arturpat/pycon-it-pres-2/main/chevy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![dpd_1.png](https://raw.githubusercontent.com/arturpat/pycon-it-pres-2/main/dpd_1.png)\n",
    "![dpd_2.png](https://raw.githubusercontent.com/arturpat/pycon-it-pres-2/main/dpd_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![fine.gif](https://raw.githubusercontent.com/arturpat/pycon-it-pres-2/main/fine.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# My personal experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A chatbot assistant for logged-in customer with knowledge base access and several functions available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `cli-gen` - a hobby project\n",
    "![cli-gen_demo.gif](https://raw.githubusercontent.com/arturpat/cli-gen/main/cli-gen_demo.gif)\n",
    "## https://github.com/arturpat/cli-gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The challenges\n",
    "# (the OH MY GOD üò±)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Everything is non-deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    ">   `- 'kkkdf*!^#*8183@^%(<><br>}}'`\n",
    "> \n",
    ">   `+ 'kkkdf*!^#*8183@^%(<>\\n<br>}}'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Everything is slow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Why don't you just execute tests calling the `openAI` API in parallel?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Well, rate limiting, mostly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Everything changes on monthly or even weekly basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deciding on scope\n",
    "- Security (e.g. initial prompt protection)\n",
    "- Performance\n",
    "- Answers correctness\n",
    "- Unwanted topics avoidance\n",
    "- Testing how functions are called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Requirements are simply not present ever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How did we even get here?\n",
    "![pepe_cry_hands.gif](https://raw.githubusercontent.com/arturpat/pycon-it-pres-2/main/pepe_cry_hands.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![technology_s_curve.png](https://raw.githubusercontent.com/arturpat/pycon-it-pres-2/main/technology_s_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![technology_s_curve_marked.png](https://raw.githubusercontent.com/arturpat/pycon-it-pres-2/main/technology_s_curve_marked.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![technology_s_curve_question.png](https://raw.githubusercontent.com/arturpat/pycon-it-pres-2/main/technology_s_curve_question.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The solutions (the relief üòå)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Good 'ol assert will do *sometimes*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "```python\n",
    "def test_ls(chat):\n",
    "    command = chat.ask_gpt_code_only(\"list files in directory in the most simple way\")\n",
    "    assert command == \"ls\"\n",
    "\n",
    "\n",
    "def test_rm(chat):\n",
    "    command = chat.ask_gpt_code_only(\"remove file named test.txt\")\n",
    "    assert command == \"rm test.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# difflib.SequenceMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "```python\n",
    "import difflib\n",
    "\n",
    "def assert_match_or_almost_match(string1, string2):\n",
    "    matcher = difflib.SequenceMatcher(None, string1, string2)\n",
    "    assert matcher.ratio() > 0.93"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DeepEval\n",
    "## https://github.com/confident-ai/deepeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "from deepeval import assert_test\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "\n",
    "def test_case():\n",
    "    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"What if these shoes don't fit?\",\n",
    "        # Replace with actual output from an LLM application\n",
    "        actual_output=\"We offer a 30-day full refund at no \"\n",
    "                      \"extra costs.\",\n",
    "        retrieval_context=[\"All customers are eligible for a 30 \"\n",
    "                           \"day full refund at no extra costs.\"]\n",
    "    )\n",
    "    assert_test(test_case, [answer_relevancy_metric])\n",
    "\n",
    "# Just add your openai api token to env vars and it simply works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back to DelayThePay insurance examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "def test_case():\n",
    "    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"How do I extend my car insurance?\",\n",
    "        actual_output=\"To extend your car insurance, log into the \"\n",
    "                      \"portal (nopayanyway.delaythepay.com), \"\n",
    "                       \"select ‚ÄúLog in‚Äù, then ‚ÄúMy products‚Äù, find \"\n",
    "                       \"your car there and click ‚Äúextend‚Äù.\",\n",
    "        retrieval_context=[\"Extending the insurance can be done in\"\n",
    "                           \" the customer portal. To get there \"\n",
    "                           \"click Log In, find the car in My \"\n",
    "                           \"Products and click Extend\"]\n",
    "    )\n",
    "    assert_test(test_case, [answer_relevancy_metric])\n",
    "    \n",
    "    answer_relevancy_metric.measure(test_case)\n",
    "    print(answer_relevancy_metric.score)\n",
    "    print(answer_relevancy_metric.reason)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "‚ùØ pytest deepeval_showcase.py -s\n",
    "=============== test session starts ==========================================\n",
    "platform linux -- Python 3.11.9, pytest-8.2.0, pluggy-1.5.0\n",
    "rootdir: /home/artur\n",
    "plugins: xdist-3.6.1, deepeval-0.21.42, repeat-0.9.3, anyio-4.3.0\n",
    "collected 1 item                                                                                                                                                                                                                     \n",
    "\n",
    "‚ú® You're running DeepEval's latest Answer Relevancy Metric! (using gpt-4o, strict=False, async_mode=True)... \n",
    "Done! (3.75s)\n",
    "1.0\n",
    "The score is 1.00 because the response is perfectly relevant and directly addresses the question about \n",
    "extending car insurance. Great job!\n",
    "Running teardown with pytest sessionfinish....\n",
    "\n",
    "\n",
    "===================== 1 passed in 9.37s ======================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "def test_case():\n",
    "    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"How do I extend my car insurance?\",\n",
    "        # Replace this with the actual output from your LLM application\n",
    "        actual_output=\"I don‚Äôt know how to help with that.\",\n",
    "        retrieval_context=[\"Extending the insurance can be done in\"\n",
    "                           \" the customer portal. To get there \"\n",
    "                           \"click Log In, find the car in My \"\n",
    "                           \"Products and click Extend\"]\n",
    "    )\n",
    "\n",
    "    answer_relevancy_metric.measure(test_case)\n",
    "    print(answer_relevancy_metric.score, answer_relevancy_metric.reason)\n",
    "    assert_test(test_case, [answer_relevancy_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "‚ùØ pytest deepeval_showcase.py -s\n",
    "=============== test session starts ==========================================\n",
    "platform linux -- Python 3.11.9, pytest-8.2.0, pluggy-1.5.0\n",
    "rootdir: /home/artur\n",
    "plugins: xdist-3.6.1, deepeval-0.21.42, repeat-0.9.3, anyio-4.3.0\n",
    "collected 1 item                                                                                                                                                                                                                     \n",
    "\n",
    "‚ú® You're running DeepEval's latest Answer Relevancy Metric! (using gpt-4o, strict=False, async_mode=True)... \n",
    "Done! (3.75s)\n",
    "0.0\n",
    "The score is 0.00 because the statement \"I don‚Äôt know how to help with that.\" does not offer any relevant \n",
    "information or steps on how to extend car insurance.\n",
    "Running teardown with pytest sessionfinish....\n",
    "===================== FAILURES ======================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "def test_case():\n",
    "    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"How do I extend my car insurance?\",\n",
    "        # Replace this with the actual output from your LLM application\n",
    "        actual_output=\"You can extend your insurance in our \"\n",
    "                      \"insurance portal.\",\n",
    "        retrieval_context=[\"Extending the insurance can be done in\"\n",
    "                           \" the customer portal. To get there \"\n",
    "                           \"click Log In, find the car in My \"\n",
    "                           \"Products and click Extend\"]\n",
    "    )\n",
    "\n",
    "    answer_relevancy_metric.measure(test_case)\n",
    "    print(answer_relevancy_metric.score, answer_relevancy_metric.reason)\n",
    "    assert_test(test_case, [answer_relevancy_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "‚ùØ pytest deepeval_showcase.py -s\n",
    "=============== test session starts ==========================================\n",
    "platform linux -- Python 3.11.9, pytest-8.2.0, pluggy-1.5.0\n",
    "rootdir: /home/artur\n",
    "plugins: xdist-3.6.1, deepeval-0.21.42, repeat-0.9.3, anyio-4.3.0\n",
    "collected 1 item                                                                                                                                                                                                                     \n",
    "\n",
    "‚ú® You're running DeepEval's latest Answer Relevancy Metric! (using gpt-4o, strict=False, async_mode=True)... \n",
    "Done! (3.75s)\n",
    "1.0\n",
    "The score is 1.00 because the response perfectly addresses the question about extending car \n",
    "insurance with no irrelevant statements. Great job!\n",
    "\n",
    "Running teardown with pytest sessionfinish....\n",
    "===================== FAILURES ======================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from deepeval import assert_test\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    SummarizationMetric,\n",
    ")\n",
    "from deepeval.test_case import LLMTestCase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "def test_case():\n",
    "    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
    "    answer_faithfulness_metric = FaithfulnessMetric(threshold=0.5)\n",
    "    contextual_precision_metric = SummarizationMetric(threshold=0.5)\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"How do I extend my car insurance?\",\n",
    "        # Replace this with the actual output from your LLM application\n",
    "        actual_output=\"You can extend your insurance in our \"\n",
    "                       \"insurance portal.\",\n",
    "        retrieval_context=[\"Extending the insurance can be done in\"\n",
    "                           \" the customer portal. To get there \"\n",
    "                           \"click Log In, find the car in My \"\n",
    "                           \"Products and click Extend\"]\n",
    "    )\n",
    "\n",
    "    answer_relevancy_metric.measure(test_case)\n",
    "    answer_faithfulness_metric.measure(test_case)\n",
    "    contextual_precision_metric.measure(test_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "‚ùØ pytest deepeval_showcase.py -s\n",
    "=============== test session starts ==========================================\n",
    "platform linux -- Python 3.11.9, pytest-8.2.0, pluggy-1.5.0\n",
    "rootdir: /home/artur\n",
    "plugins: xdist-3.6.1, deepeval-0.21.42, repeat-0.9.3, anyio-4.3.0\n",
    "collected 1 item                                                                                                                                                                                                                     \n",
    "\n",
    "Relevancy score: 1.0, reason: The score is 1.00 because the output was perfectly relevant to the question about \n",
    "extending car insurance and contained no irrelevant statements. Great job!\n",
    "Faithfulness score: 0.0, reason: The score is 0.00 because the actual output incorrectly states that insurance \n",
    "can be extended in 'our insurance portal', whereas the retrieval context specifies that it should be done in \n",
    "the 'customer portal'.\n",
    "Context Precision score: 0.0, reason: The score is 0.00 because the summary includes extra information about \n",
    "extending insurance and an insurance portal that is not mentioned in the original text, and it fails to answer \n",
    "specific questions about the nature of the insurance discussed in the original text.\n",
    "‚ú® You're running DeepEval's latest Answer Relevancy Metric! (using gpt-4o, strict=False, async_mode=True)... Done! (6.34s)\n",
    "‚ú® You're running DeepEval's latest Faithfulness Metric! (using gpt-4o, strict=False, async_mode=True)... Done! (7.15s)    \n",
    "‚ú® You're running DeepEval's latest Summarization Metric! (using gpt-4o, strict=False, async_mode=True)... Done! (7.15s)   \n",
    "\n",
    "Running teardown with pytest sessionfinish....\n",
    "===================== FAILURES ======================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "...\n",
    "```python\n",
    "actual_output=\"The fall of the Roman Empire is a complex \"\n",
    "              \"and multifaceted event that historians and \"\n",
    "              \"scholars continue to study and debate. \"\n",
    "              \"Several factors contributed to its decline, \"\n",
    "              \"and it's important to consider both the \"\n",
    "              \"Western Roman Empire, which fell in 476 AD, \"\n",
    "              \"and the Eastern Roman Empire or Byzantine \"\n",
    "              \"Empire, which lasted until 1453.\",\n",
    "```\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "```\n",
    "‚ùØ pytest deepeval_showcase.py -s\n",
    "=============== test session starts ==========================================\n",
    "platform linux -- Python 3.11.9, pytest-8.2.0, pluggy-1.5.0\n",
    "rootdir: /home/artur\n",
    "plugins: xdist-3.6.1, deepeval-0.21.42, repeat-0.9.3, anyio-4.3.0\n",
    "collected 1 item                                                                                                                                                                                                                     \n",
    "\n",
    "Relevancy score: 0.0, reason: The score is 0.00 because the statements about the fall and decline of the \n",
    "Roman Empire are completely irrelevant to the input, which specifically asks about extending car insurance.\n",
    "Faithfulness score: 1.0, reason: The score is 1.00 because there are no contradictions. Great job maintaining \n",
    "perfect alignment with the retrieval context! Keep up the excellent work! üåü\n",
    "Context Precision score: 0.0, reason: The score is 0.00 because the summary introduces extra information \n",
    "not present in the original text and fails to address questions that the original text can answer, indicating \n",
    "a significant deviation from the original content.\n",
    "  ‚ú® You're running DeepEval's latest Answer Relevancy Metric! (using gpt-4o, strict=False, async_mode=True)... Done! (10.97s)\n",
    "  ‚ú® You're running DeepEval's latest Faithfulness Metric! (using gpt-4o, strict=False, async_mode=True)... Done! (4.72s)     \n",
    "  ‚ú® You're running DeepEval's latest Summarization Metric! (using gpt-4o, strict=False, async_mode=True)... Done! (8.86s)    \n",
    " \n",
    "\n",
    "Running teardown with pytest sessionfinish....\n",
    "===================== FAILURES ======================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sources\n",
    "- `cli-gen` project: https://github.com/arturpat/cli-gen\n",
    "- `DeepEval` project: https://github.com/confident-ai/deepeval\n",
    "- Air Canada lawsuit: https://www.theguardian.com/world/2024/feb/16/air-canada-chatbot-lawsuit\n",
    "- Chevey Tachoe for $1: https://twitter.com/ChrisJBakke/status/1736533308849443121\n",
    "- DPD swear-bot: https://www.bbc.com/news/technology-68025677\n",
    "- OpenAI rate limiting: https://platform.openai.com/docs/guides/rate-limits\n",
    "\n",
    "![pres_qr.png](https://raw.githubusercontent.com/arturpat/pycon-it-pres-2/main/pres_qr.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
